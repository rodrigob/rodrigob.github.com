---
Classification:
- :group: Classification
  :name: MNIST
  :evaluation_units: error %
  :description: "[Classify handwriten digits](http://yann.lecun.com/exdb/mnist/).
    \nSome additional results are available on the [original dataset page](http://yann.lecun.com/exdb/mnist/)."
  :figure_url: mnist.png
- :group: Classification
  :name: CIFAR-10
  :evaluation_units: accuracy %
  :description: Classify [32x32 colour images](http://www.cs.toronto.edu/~kriz/cifar.html).
  :figure_url: cifar_10.png
- :group: Classification
  :name: CIFAR-100
  :evaluation_units: accuracy %
  :description: Classify [32x32 colour images](http://www.cs.toronto.edu/~kriz/cifar.html).
  :figure_url: cifar_100.png
- :group: Classification
  :name: STL-10
  :evaluation_units: accuracy %
  :description: Similar to CIFAR-10 but with 96x96 images. [Original dataset website](http://www.stanford.edu/~acoates/stl10/).
  :figure_url: stl_10.png
- :group: Classification
  :name: SVHN
  :evaluation_units: error %
  :description: |
    [The Street View House Numbers (SVHN) Dataset](http://ufldl.stanford.edu/housenumbers).

    SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data preprocessing and formatting. It can be seen as similar in flavor to MNIST(e.g., the images are of small cropped digits), but incorporates an order of magnitude more labeled data (over 600,000 digit images) and comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images.
  :figure_url: http://ufldl.stanford.edu/housenumbers/32x32eg.png
- :group: Classification
  :name: ILSVRC2012 task 1
  :evaluation_units: Error (5 guesses)
  :description: |-
    1000 categories [classification challenge](http://www.image-net.org/challenges/LSVRC/2012/index). With tens of thousands of training, validation and testing images.

    See this interesting [comparative analysis](http://www.image-net.org/challenges/LSVRC/2012/analysis/).
  :figure_url: ilsvrc2012_task1.png
  :external_results_url: http://www.image-net.org/challenges/LSVRC/2012/results.html#t1
- :group: Classification
  :name: GIFGIF
  :evaluation_units: nMSE
  :description: |-
    Over 6000 annotated animated GIFs with 17 emotional labels.
  :figure_url: gifgif.png
- :group: Classification
  :name: Video Emotion Dataset
  :evaluation_units: accuracy %
  :description: |-
    Contains 1,101 videos from YouTube and Flickr with an average duration of 107 seconds. It is manfully annotated into 8 basic emotion categories according to the Plutchik’s wheel of Emotions, including "anger", "anticipation", "disgust", "fear", "sadness", "joy", "surprise" and "trust".
  :figure_url: video-emotion-dataset.png
- :group: Classification
  :name: Ekman Emotion Dataset
  :evaluation_units: accuracy %
  :description: |-
    Contains 1,637 videos labeled into Ekman’s 6 basic emotions (joy, sadness, anger, fear, disgust, surprise) by 10 volunteers with a minimum number of 221 videos per category.
  :figure_url: ekman-emotion-video.png
- :group: Classification
  :name: NTU RGB+D
  :evaluation_units: accuracy %
  :description: |-
    This dataset consists of 56,880 action samples containing 4 different modalities (RGB videos, depth map sequences, 3D skeletal data, infrared videos) of data for each sample and [60 classes of actions](https://github.com/shahroudy/NTURGB-D).
  :figure_url: ntu-rgbd.png
Detection:
- :group: Detection
  :name: Pascal VOC 2007 comp3
  :evaluation_units: 'mAP percent '
  :description: Pascal VOC 2007 is commonly used because the test set has been realased.
    comp3 is the objects detection competition, using only the comp3 pascal training
    data.
  :figure_url: pascal_voc_2007.png
- :group: Detection
  :name: Pascal VOC 2007 comp4
  :evaluation_units: 'mAP percent '
  :description: Just like comp3 but "any training data" can be used.
  :figure_url: pascal_voc_2007.png
- :group: Detection
  :name: Pascal VOC 2010 comp3
  :evaluation_units: 'mAP percent '
  :description: Pascal VOC 2010 version of the challenge. comp3 is the objects detection
    competition.
  :figure_url: pascal_voc_2010.png
- :group: Detection
  :name: Pascal VOC 2010 comp4
  :evaluation_units: 'mAP percent '
  :description: Just like comp3 but "any training data" can be used.
  :figure_url: pascal_voc_2010.png
- :group: Detection
  :name: Pascal VOC 2011 comp3
  :evaluation_units: 'mAP percent '
  :description: Last Pascal VOC challenge instance (2012 version had identical data).
  :figure_url: pascal_voc_2012.png
- :group: Detection
  :name: Caltech Pedestrians USA
  :evaluation_units: average miss-rate %
  :description: "[Project website](http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/)."
  :figure_url: http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/files/peds02_web.jpg
  :external_results_url: http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/rocs/UsaTestRocReasonable.pdf
- :group: Detection
  :name: INRIA Persons
  :evaluation_units: average miss-rate %
  :description: |-
    Evaluated using the [Caltech Pedestrians toolkit](http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/).
    [Original dataset website](http://pascal.inrialpes.fr/data/human/).
  :figure_url: inria_persons.png
  :external_results_url: http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/rocs/InriaTestRocReasonable.pdf
- :group: Detection
  :name: 'ETH Pedestrian '
  :evaluation_units: average miss-rate %
  :description: |-
    Evaluated using the [Caltech Pedestrians toolkit](http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/).
    Only left images used.
    [Original dataset website](http://www.vision.ee.ethz.ch/~aess/dataset/).
  :figure_url: eth_pedestrian.png
  :external_results_url: http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/rocs/ETHRocReasonable.pdf
- :group: Detection
  :name: TUD-Brussels Pedestrian
  :evaluation_units: average miss-rate %
  :description: |-
    Evaluated using the [Caltech Pedestrians toolkit](http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/).
    [Original dataset website](http://www.d2.mpi-inf.mpg.de/tud-brussels).
  :figure_url: http://www.d2.mpi-inf.mpg.de/sites/default/files/datasets/tud-brussels/teaser.png
  :external_results_url: http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/rocs/TudBrusselsRocReasonable.pdf
- :group: Detection
  :name: Daimler Pedestrian
  :evaluation_units: average miss-rate %
  :description: |-
    Evaluated using the [Caltech Pedestrians toolkit](http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/).
    [Original dataset website](http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Daimler_Mono_Ped__Detection_Be/daimler_mono_ped__detection_be.html).
  :figure_url: http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/ped_det_benchmark.jpg
  :external_results_url: http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/rocs/DaimlerRocReasonable.pdf
- :group: Detection
  :name: KITTI Vision Benchmark
  :evaluation_units: average recall %
  :description: A rich dataset to evaluate multiple computer vision tasks, including
    cars, pedestrian and bycicles detection.
  :figure_url: http://www.cvlibs.net/datasets/kitti/video/kitti_trailer.jpg
  :external_results_url: http://www.cvlibs.net/datasets/kitti/eval_object.php
Pose estimation:
- :group: Pose estimation
  :name: Leeds Sport Poses
  :evaluation_units: PCP %
  :description: "[2000 poses anotated pictures](http://www.comp.leeds.ac.uk/mat4saj/lsp.html)
    from Flickr. From a selected set of activities and with the person at the center
    of the pictures."
  :figure_url: leeds_sport_poses.jpg
Semantic labeling:
- :group: Semantic labeling
  :name: MSRC-21
  :evaluation_units: accuracy % per-class / (and) per-pixel
  :description: "One of the oldest and classic dataset for semantic labelling.\n21
    different categories of surfaces are considered.\nDespite the innacuracies in
    the annotations and how unbalanced the classes are, this dataset still is commonly
    used as reference point.\nNote that here we consider the [original annotations](http://jamie.shotton.org/work/data.html)
    (where most results are published), not the [cleaned-up version](http://www.cs.cmu.edu/~tmalisie/projects/bmvc07).
    \n\n\nThe results are reported per-class and per-pixel (this is sometimes called
    \"average\" and \"global\" result, respectively).\n\n\n[Original dataset website](http://jamie.shotton.org/work/data.html)\n"
  :figure_url: msrc_21.png
Saliency/Segmentation:
- :group: Saliency/Segmentation
  :name: Salient Object Detection benchmark
  :evaluation_units: AUC (precision/recall area under the curve) and MAE (mean absolute
    error)
  :description: This benchmark aggregates results from 36 methods over five datasets
    (MSRA10K, ECSSD, THUR15K, JuddDB, and DUTOMRON).
  :figure_url: saliency_benchmark.png
  :external_results_url: http://mmcheng.net/salobjbenchmark/
