
# Palette used:
# Steady end-node: #73d216 (chameleon mid)
# Good end-node: #8ae234   (chameleon highlight)
# Uncertain end-node: #e9b96e (chocolate higlight)
# Bad end-node: #ef2929	(scalet red highlight)
# State node: #d3d7cf	(aluminium mid)


digraph G {

  // x and y min-nodes-distance, defaults: 0.25 & 0.5
  nodesep="0.20" 
  ranksep="0.25" 
    
  graph [
    label="Map of AGI futures"
    labelloc = t
    labeljust = l
	fontname = "Helvetica,Arial,sans-serif"
	fontsize = 30
	layout = dot
	rankdir = TB
	newrank = true
	#splines=ortho
	#splines=curved
	splines=true
  ]  
  
  node [
        id="\N" // use the node name as svg id
        href="#\N"
	//	style=filled
	//	shape=rect
    	penwidth=3
    	color="#d9d9d9"
	//	pencolor="#00000044" // frames color
		fontname="Helvetica,Arial,sans-serif"
        fontsize=20 // 14 is default
	//	shape=plaintext
	    shape=box
	]

    edge [
        // use the nodes name as svg id; 
        // this might not be unique if graph has two similar arrows
        // \E includes edge position information :c, :sw, thus we use \T and \H instead.
        id="\T->\H"
        href="#\T" // we point to source node in FAQ, since it provides support for the choices.
    //	arrowsize=0.5
    	fontname="Helvetica,Arial,sans-serif"
        fontsize=16 // 14 is default
    //	labeldistance=3
    //	labelfontcolor="#00000080"
    	penwidth=2
    //	style=dotted // dotted style symbolizes data transfer
      
    ]


  subgraph cluster_outcomes {
    graph[
    #style=filled
    #color=lightgrey
    style=dotted
    #color=lightblue
    label="Outcome probabilities";
    fontsize = 20
    labelloc=t
    labeljust=l
    # margin="8,8"

    ]

  {
  
  node[
      color=grey
      fillcolor=none
      shape=box
      style=filled
      fontsize=18
      class="outcome-node"
  ] 

    outcomes [
        shape=plaintext
        label=< 
        <table cellspacing="0">
        <tr><td align="left">Good future</td><td width="120" id="good_future_value">%</td></tr>
        <tr><td align="left">Steady future</td><td id="steady_future_value">%</td></tr>
        <tr><td align="left">Hazy future</td><td id="uncertain_future_value">%</td></tr>
        <tr><td align="left">Bad future</td><td id="bad_future_value">%</td></tr>
        </table> 
        >
    ]

  }

  }



  // stylesheet = "https://g3doc.corp.google.com/frameworks/g3doc/includes/graphviz-style.css"
  can_agi_exist
  [
    label="Can superhuman\nArtifical General Intelligence exist?"
    //shape=diamond
    shape=egg
    // shape=trapezium
    // shape=plain
    //shape=oval
    //shape=box
    style=filled fillcolor="#bafffc" 
    color=none 
    class="start-node"
  ]

  
  
  {
      
  node[
      color=none
      //shape=rectangle
      shape=box
      //shape=egg
      style=filled
  ]
  
  ai_this_century [
    class="state-node"
    label= "AGI this century"
    fillcolor=lightgray
   ]

  {
  node[
    fillcolor="#e9b96e"
   class="end-node uncertain-future"
   ]
  second_chance_for_ai [
  ]
  
 conflicting_ais [label="Conflicting AGIs" ]
  }

  {
  node[
   fillcolor="#ff8f8f"
   class="end-node bad-future"
   ]
  astro_suffering
  medium_scale_dystopia
  ai_kills_humans
  aligned_ai_human_extinction
  authoritarian_dystopia
  }

  {
  node[
   class="end-node steady-future"
   ]
  no_ai_this_century [fillcolor="#73d216" ]
  business_as_usual [ label= "Business as usual" fillcolor="#73d216" ]
  }
  
  # agi_utopia [ label="AGI Utopia" fillcolor="#94ff8f"]
  agi_utopia [ 
    label="Beneficial AGI" 
    fillcolor="#8ae234"
    class="end-node good-future"
    ]
  # "Peaceful human extinction"[fillcolor="#e3b591"]
  
  {
  node[
   class="state-node"
   fillcolor=lightgray
   ]
 created_inefectual_ai 
 humans_killed_harmfull_ai 
 offed_misaligned_ai 
 }
 

  
  
  }
  
  will_ai_collaborate_with_ai [label="Will AGIs colaborate\nwith each other?" ]
  can_agi_exist_this_century [label="...in this century?"]
  
  can_agi_exist -> can_agi_exist_this_century [
      label=<&nbsp;yes; 99>
      #labeldistance=3
      weight=3 
      penwidth=4]
      
      
  no_ai_this_century [label="No AGI this century"]
  
  can_agi_exist -> no_ai_this_century [
      taillabel=<no; 1> 
      labeldistance=3
      tailport=c
      headport=e
      penwidth=1 
      weight=0
      ]
 can_agi_exist_this_century -> can_we_agree_no_ai [
     label=<&nbsp;yes; 70> 
     weight=10 
     penwidth=3]
  can_agi_exist_this_century -> no_ai_this_century [
      taillabel=<&nbsp;no; 30> 
      labeldistance=3.5
      tailport=e
      #headport=c
      penwidth=2
      weight=1
      ]
  is_alignment_problem_solved -> is_agi_aligned [
      label=<&nbsp;yes; 70> 
      #labeldistance=0
      tailport=s
      headport=n
      weight=10 
      penwidth=3]
 is_alignment_problem_solved -> can_we_detect_misaligned [
      taillabel=<&nbsp;no; 30> 
      labeldistance=6
      tailport=w
      headport=n
      weight=5
      penwidth=2]
  
  
  {rank=same can_we_agree_no_ai can_we_enforce_no_ai}
  
  can_we_agree_no_ai [label="Can we agree not to build it?"]
  can_we_enforce_no_ai [label= "Can we enforce it?"]
  
  can_we_agree_no_ai -> can_we_enforce_no_ai [
      label=<&nbsp;yes; 15> 
      #headport=c
      #tailport=c
      weight=1
      penwidth=1]
  can_we_agree_no_ai -> ai_this_century [
      label=<&nbsp;no; 85> 
      weight=20 
      penwidth=4]

  can_we_enforce_no_ai -> no_ai_this_century [
      taillabel=<&nbsp;yes; 1>
      labeldistance=2.5
      #headport=ne
      tailport=se
      weight=1
      penwidth=1]

  someone_wants_to_build_it[label="Will there be someone\nwilling to build it?"]
  can_we_enforce_no_ai -> someone_wants_to_build_it [
      label=<&nbsp;no; 99> 
      weight=8
      penwidth=4]
 
  someone_wants_to_build_it -> ai_this_century [
      label=<&nbsp;yes; 95> 
      tailport=sw
      headport=c
      weight=1 
      penwidth=4]
  someone_wants_to_build_it -> no_ai_this_century [
      label=<&nbsp;no; 5> 
      #headport=c
      #tailport=c
      weight=10
      penwidth=1]
  
  # "AGI this century" -> "Can AGI be aligned?" [weight=10 penwidth=5]

  is_agi_transformative[label="Is AGI transformative?"]
  {rank=same is_agi_transformative business_as_usual }
  ai_this_century -> is_agi_transformative[
      weight=5
      #minlen=1
      ]
  
  first_ai_kills_other_ais [label="Will the first AGI\nprevent second AGI to appear?"]
  
  is_agi_transformative -> first_ai_kills_other_ais [
      label=<&nbsp;yes; 99> 
      weight=15
      penwidth=3
      ]
  is_agi_transformative -> business_as_usual [
      label=<&nbsp;no; 1>
      tailport=c
      headport=c
      weight=1
    ]
  
  is_alignment_problem_solved [label="Have we solved\nAGI alignment in theory?"]
  
  first_ai_kills_other_ais -> is_alignment_problem_solved [
      label=<&nbsp;yes; 80> 
  weight=20
  penwidth=4
  ] 
  
  
  #{rank=same first_ai_kills_other_ais will_ai_collaborate_with_ai conflicting_ais}
  {rank=same first_ai_kills_other_ais will_ai_collaborate_with_ai}
   first_ai_kills_other_ais -> will_ai_collaborate_with_ai [
      label=<&nbsp;no; 20>
      #tailport=e
      #headport=n
      penwidth=1
      weight=1
      ]
 # first_ai_kills_other_ais -> "It's complicated..." [label=no]

  
  
  will_ai_collaborate_with_ai -> is_alignment_problem_solved [
    label=<&nbsp;yes; 10>
    penwidth=1
    weight=5
    tailport=sw
    headport=e
   ]
  
   #{rank=same will_ai_collaborate_with_ai conflicting_ais}
    will_ai_collaborate_with_ai -> conflicting_ais [
 
    label=<&nbsp;no; 90>
    penwidth=4
    weight=1
    #tailport=w
    #headport=c
   ]
 
  do_they_have_good_intentions [label="Do they have good\nintentions for humanity?"]
  are_they_thoughful [label="Are the controlers thoughtful of\nunintended consequences?"]
   
  aligned_to_whom [ label="Aligned to whom?" ]
  is_agi_aligned [label="Is the AGI aligned?"]

  is_agi_aligned -> agi_protects_users [
      label=<&nbsp;yes; 60> 
      #labeldistance=3.5
      #labelangle=25
      # this weight is crucial for the overall layout
      weight=10
      #weight=80 
      penwidth=3]
      
  is_agi_aligned -> can_we_detect_misaligned[
      label=<&nbsp;no; 40> 
      tailport=w
      headport=c
      weight=1
      penwidth=2
      ]
      
  
  subgraph cluster_aligned {
    graph [style="dashed"
            label="Aligned AGI"
            fontsize = 20
            labelloc=t
            labeljust=r
            ]
            
            
    aligned_to_whom -> agi_utopia [
        label=<&nbsp;All humans; 40> 
        weight=130 
        penwidth=2
    ]
    aligned_to_whom -> do_they_have_good_intentions [
        label=<&nbsp;Some humans; 60> 
        #tail
        #labeldistance=8
        #labelangle=15
        tailport=c
        headport=n
        weight=1
        penwidth=3]
    #"Aligned to whom?" -> "Does AGI decide to\nextinguish humans?" [label=<&nbsp;All sentient beings; 3> weight=1 penwidth=1]
    
    #"Does AGI decide to\nextinguish humans?" -> "Peaceful human extinction" [label=<yes; 20> penwidth=1]
    #"Does AGI decide to\nextinguish humans?" -> "AGI Utopia" [label=<no; 80> weight=80 penwidth=4]
 


    
    #agi_protects_users [label="Does the AGI protect its controlers?"]
    agi_protects_users [label="Follows values or instructions?"]
    
    agi_protects_users -> aligned_to_whom  [
        #label=<&nbsp;yes; 60>
        label=<&nbsp;values; 60> 
    tailport=s
    headport=n
    weight=200]
    
    agi_protects_users -> are_they_thoughful  [
        #label=<&nbsp;no; 40>
        label=<&nbsp;instructions; 40>
    tailport=e
    headport=n
    weight=1
    ]
 
    aligned_ai_human_extinction[ label="Shoot-in-foot\nhuman extinction" ]
    are_they_thoughful ->  aligned_ai_human_extinction [label=<&nbsp;no; 50>  weight=300]
    are_they_thoughful ->  aligned_to_whom [
        label=<&nbsp;yes; 50>  weight=1]
    
    do_they_have_good_intentions -> agi_utopia [
        label=<&nbsp;yes; 40>
        weight=0
        ]
    
    authoritarian_dystopia[label="Authoritarian dystopia"]
    do_they_have_good_intentions -> authoritarian_dystopia [
        label=<&nbsp;no; 60>
        ]
  }
  
 
  
  subgraph cluster_misaligned {
      
    
      
    graph [style="dashed"
    label="Misaligned AGI"
    fontsize = 20
    labelloc=t
    labeljust=l
    ]
    
    can_we_off_it [ label= "Can we easily turn it off?" ]
    
     
    can_we_detect_misaligned [label="Can we know it's misaligned?"]
    
    can_we_detect_misaligned -> can_we_off_it [
        label=<&nbsp;yes; 30> 
        tailport=w
        headport=n
        weight=2
        #tailport=s
        penwidth=2]
    
    
    ai_harms_humans [label="Is it harming us?"]
    
    can_we_detect_misaligned -> ai_harms_humans [
        label=<&nbsp;no; 70> 
        #tailport=s
        headport=e
        weight=1
        penwidth=3]
    
    
    
    can_we_off_it ->  ai_harms_humans [
        label=<&nbsp;no; 80>
    weight=10
    penwidth=4]
  
    humans_can_kill_ai [label="Can we neutralize it?"]
    
    
    ai_harms_humans -> humans_can_kill_ai [
        label=<&nbsp;yes; 80> 
        weight=15
        penwidth=4]
    
    wants_human_exctinction [label= "Does it benefit from our extinction?"]
    wants_human_suffering [label= "Does is benefit from our suffering?"]
    ai_kills_humans [label= "AGI-initiated\nhuman extinction"]
    
    
    humans_can_kill_ai -> wants_human_exctinction [
        label=<&nbsp;no; 70>
        weight=15
        penwidth=3]
 
    {rank=same wants_human_exctinction ai_kills_humans}
    wants_human_exctinction -> ai_kills_humans  [
        label=<&nbsp;yes; 50>
        weight=1
        #tailport=e
        #headport=w
        penwidth=2
        ]
        
        
    wants_human_exctinction -> wants_human_suffering [
        label=<&nbsp;no; 50> weight=1 penwidth=2]

    medium_scale_dystopia [ label= "Medium scale dystopia" ]
    wants_human_suffering -> medium_scale_dystopia [label=<&nbsp;no; 99> 
    penwidth=4
    tailport=s
    headport=n
    weight=2 
    
    ]

   astro_suffering [label="Astronomical\nsuffering"] 

   {rank=same wants_human_suffering astro_suffering }
   
    wants_human_suffering -> astro_suffering [
        label=<&nbsp;yes; 1> 
        penwidth=1
    #tailport=se
    #headport=c
    weight=1 
        ]


  }
  
  
  offed_misaligned_ai [label="Turned off misaligned AGI" ] 
  created_inefectual_ai [label="Harmless misaligned AGI exists" ]
  humans_killed_harmfull_ai [label="We neutralized harmfull AGI"]

  {rank=same can_we_off_it offed_misaligned_ai }
  {rank=same ai_harms_humans created_inefectual_ai}
  {rank=same humans_can_kill_ai humans_killed_harmfull_ai}

      
  {offed_misaligned_ai -> created_inefectual_ai -> humans_killed_harmfull_ai [
      style=invis
      weight=10
      ] }
     
  can_we_off_it -> offed_misaligned_ai [
        taillabel=<&nbsp;yes; 20>
        labeldistance=3.5
        labelangle=30
        tailport=w
        headport=c
        weight=2
        #tailport=sw
        penwidth=1]
    
  

  
  ai_harms_humans -> created_inefectual_ai [
      
      taillabel=<no; 20>
      labeldistance=3.5
      labelangle=30
      #xlabel=<We live<br/>harmless but misaligned AGI>
      #samehead=build_or_not_build 
      tailport=w
      headport=c
      weight=2
      penwidth=1]
  
  
 
  
  #"Can we agree not to build it?" [
  #    taillabel=<no; 20>
  #    labeldistance=3.5
  #    labelangle=20
  #    #xlabel=<We live<br/>harmless but misaligned AGI>
  #    #samehead=build_or_not_build 
  #    #headport=sw
  #    #headport=w
  #    #tailport=w
  #    weight=2
  #    penwidth=1]
  
 

  humans_can_kill_ai ->  humans_killed_harmfull_ai [
      taillabel=<yes; 30> 
      labeldistance=3.5
      labelangle=40
      #xlabel=<We destroyed<br/>bad AGI>
      #samehead=build_or_not_build 
      #headport=sw
      headport=c
      tailport=w
      weight=2
      penwidth=2
      ]
      
      
     
    offed_misaligned_ai -> second_chance_for_ai  [
    tailport=sw
    #tailport=s
    #headport=c
    headport=nw
    #samehead=created_inefectual_ai
    weight=0
    ] 
    
    created_inefectual_ai -> second_chance_for_ai [
     #tailport=w
    tailport=sw
    #headport=n
    #headport=c
    weight=1
    ]
    
    humans_killed_harmfull_ai -> second_chance_for_ai [
     tailport=s
     #tailport=s
     headport=n
     weight=10
     ]      
  
     
  second_chance_for_ai [label="Second chance at building AGI" ]
   
  second_chance_for_ai -> can_we_agree_no_ai [
      #taillabel=<yes; 20> 
      xlabel="Second chance at building AGI"
      #labeldistance=6
      style=dashed
      #labeldistance=4
      #labelangle=40
      #xlabel=<We turned-off<br/>bad AGI>
      #samehead=build_or_not_build 
      #tailport=nw
      #tailport=w
      headport=w
      #headport=sw
      #weight=50
      weight=1
      penwidth=1
      ]
  
  
  subgraph cluster_legend {



    graph[
    #style=filled
    #color=lightgrey
    style=dotted
    #color=lightblue
    label="Nodes legend";
    fontsize=16
    labelloc=t
    labeljust=l
    #margin="36,8"


    ]

  node[
      color=none
      shape=box
      style=filled
      class="legend-node"
      fontsize=14 
      # default fontsize is 14
  ] 

    edge [
        weight=5
    ]

    legend_start [class="start-node legend-node" label="Start"]
    legend_start [ 
        color=none
        shape=egg
        // shape=oval
        fillcolor="#bafffc" ]       
        
    da [ 
        color=lightgray
        fillcolor=none ] 
        
    db [ 
        color=lightgray
        fillcolor=none ] 
        
    state  [ label="State" 
    class="state-node legend-node" 
    fillcolor=lightgray ]   
    
    unchanged_future [ class="unchanged-future legend-node" label= "Steady future" fillcolor="#73d216" ] 
    good_future [ class="good-future legend-node" label="Good future"  fillcolor="#8ae234" ] 
    uncertain_future[ class="uncertain-future legend-node" label="Hazy future"  fillcolor="#e9b96e" ] 
    bad_future[class="bad-future legend-node" label="Bad future"  fillcolor="#ff8f8f" ] 

    da [ label= "Bifurcation A"]
    db [ label= "Bifurcation B"] 
    
    legend_start -> da -> state -> db [weight=2]
   
    da -> unchanged_future [tailport=e headport=c weight=0]
    
     { rank=same
    db -> unchanged_future 
         
     }
 
    db -> good_future
    db -> uncertain_future
    db -> bad_future
    
  }

  
  
  
}
